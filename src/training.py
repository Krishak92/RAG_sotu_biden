import torch
import os
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments
from datasets import Dataset

def align_transcriptions(transcription1, transcription2, filler="[MISSING]"):
    """
    Aligne deux transcriptions en ajoutant des passages de remplissage à la plus courte.

    Args:
        transcription1 (list): Liste de passages de la première transcription.
        transcription2 (list): Liste de passages de la deuxième transcription.
        filler (str): Texte à ajouter comme passage de remplissage.

    Returns:
        tuple: Deux listes de transcriptions alignées avec la même longueur.
    """
    len1, len2 = len(transcription1), len(transcription2)
    
    if len1 < len2:
        # Compléter transcription1 avec des fillers
        transcription1.extend([filler] * (len2 - len1))
    elif len2 < len1:
        # Compléter transcription2 avec des fillers
        transcription2.extend([filler] * (len1 - len2))
    
    return transcription1, transcription2

def prepare_data_for_training(passages_official, passages_autogenerated):
    """
    Prépare les données d'entraînement à partir des passages officiels et autogénérés.

    Args:
        passages_official (list): Liste de passages issus de la transcription officielle.
        passages_autogenerated (list): Liste de passages issus de la transcription autogénérée.

    Returns:
        Dataset: Un objet Dataset de Hugging Face contenant les données d'entrée et les cibles.
    """
    if len(passages_official) != len(passages_autogenerated):
        aligned_official, aligned_autogenerated = align_transcriptions(passages_official, passages_autogenerated)

    data = {
        "input_text": aligned_official,
        "output_text": aligned_autogenerated
    }
    
    dataset = Dataset.from_dict(data)
    return dataset

def fine_tune_model(dataset, model_name="t5-small", output_dir="models/fine_tuned_t5", epochs=3, batch_size=8):
    """
    Fine-tune un modèle génératif pré-entraîné sur un dataset donné.

    Args:
        dataset (Dataset): Ensemble de données pour l'entraînement.
        model_name (str): Nom du modèle pré-entraîné à utiliser.
        output_dir (str): Répertoire pour sauvegarder le modèle fine-tuné.
        epochs (int): Nombre d'époques d'entraînement.
        batch_size (int): Taille des lots pour l'entraînement.

    Returns:
        Trainer: Objet Trainer contenant le modèle fine-tuné.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    def preprocess_function(examples):
        inputs = tokenizer(examples["input_text"], max_length=512, truncation=True, padding="max_length")
        outputs = tokenizer(examples["output_text"], max_length=512, truncation=True, padding="max_length")
        inputs["labels"] = outputs["input_ids"]
        return inputs

    tokenized_dataset = dataset.map(preprocess_function, batched=True)

    training_args = TrainingArguments(
        output_dir=output_dir,
        eval_strategy="no",
        learning_rate=2e-5,
        per_device_train_batch_size=batch_size,
        num_train_epochs=epochs,
        weight_decay=0.01,
        save_total_limit=3,
        save_steps=10,
        logging_dir=f"{output_dir}/logs",
        logging_steps=10
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset
    )

    # Crée le répertoire de sauvegarde si nécessaire
    os.makedirs(output_dir, exist_ok=True)

    # Entraînement
    trainer.train()

    # Sauvegarde du modèle et du tokenizer
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"Modèle fine-tuné sauvegardé dans : {output_dir}")

    return trainer

