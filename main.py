import faiss
import numpy as np
import re
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments
from src.preprocessing import normalize_text
from src.indexation import split_into_passages, encode_passages, build_faiss_index
from src.training import prepare_data_for_training, fine_tune_model
from src.pipeline import pipeline
from src.test import test_pipeline

def load_text(file_path):
    """
    Load raw text from a file

    Args:
        file_path (str): Path of the file.

    Returns:
        str: Content of the file as chains of character.
    """
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

def main():
    
    official_transcript_path = "data/biden-sotu-2023-planned-official.txt"
    autogenerated_transcript_path = "data/biden-sotu-2023-autogenerated-transcript.txt"

    #loading texts
    official_text = load_text(official_transcript_path)
    autogenerated_text = load_text(autogenerated_transcript_path)
    # combined_text = official_text + "\n" + autogenerated_text

    #preprocessing
    normalized_official_transcript = normalize_text(official_text)
    normalized_autogenerated_transcript = normalize_text(autogenerated_text)

    with open("normalized_official_transcript.txt", 'w', encoding='utf-8') as file:
        file.write(normalized_official_transcript)

    with open("normalized_autogenerated_transcript.txt", 'w', encoding='utf-8') as file:
        file.write(normalized_autogenerated_transcript)

    print("Normalization over. Normalized files have been saved.")

    #split
    passages_official = split_into_passages(normalized_official_transcript, passage_size=100)
    passages_autogenerated = split_into_passages(normalized_autogenerated_transcript, passage_size=100)

    #encode
    embeddings_official = encode_passages(passages_official)
    embeddings_autogenerated = encode_passages(passages_autogenerated)

    #index construction
    index_official = build_faiss_index(embeddings_official)
    index_autogenerated = build_faiss_index(embeddings_autogenerated)

    #save
    np.save("passages_official.npy", passages_official)
    np.save("passages_autogenerated.npy", passages_autogenerated)
    faiss.write_index(index_official, "faiss_index_official.bin")
    faiss.write_index(index_autogenerated, "faiss_index_autogenerated.bin")

    print("FAISS index created and successfully saved!")

    #training
    #data preparation for the generative model
    dataset = prepare_data_for_training(passages_official, passages_autogenerated)

    #fine-tuning of the generative model
    fine_tune_model(
        dataset=dataset,
        model_name="t5-small",
        output_dir="models/fine_tuned_t5"
    )

    print("Fine-tuning complete. Model saved in 'models/fine_tuned_t5'.")

    question = "What are the main themes mentioned in the speech?"
    faiss_index_path = "faiss_index_official.bin"
    passages_path = "passages_official.npy"
    
    response = pipeline(question, faiss_index_path, passages_path)
    print("Generated answer:", response)

    query = "What are the main topics about education in the speech?"
    model_name = "t5-small"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    sentence_encoder = SentenceTransformer("all-MiniLM-L6-v2")
    test_pipeline(
        faiss_index=index_official,          #index FAISS contenant les passages encodés
        passages=passages_official,          #liste des passages correspondants
        encoder_model=sentence_encoder,      #modèle SentenceTransformer utilisé pour l'encodage
        generative_model=model,           #modèle génératif (T5 ou autre)
        tokenizer=tokenizer,              #tokenizer correspondant au modèle génératif
        query=query,                         #question à poser
        top_k=5                              #nombre de passages à récupérer
    )

if __name__ == "__main__":
    main()